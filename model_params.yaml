params:
  Linear Regression:
    fit_intercept: [True, False]
    copy_X: [True, False]
    n_jobs: [-1, 1, None]
  K-Neighbors Regressor: 
    n_neighbors: [3, 5, 7, 9]
    weights: ['uniform', 'distance']
    algorithm: ['auto', 'ball_tree', 'kd_tree', 'brute']
    leaf_size: [10, 20, 30]
    p: [1, 2]
    metric: ['minkowski', 'euclidean', 'manhattan']
  Decision Tree:
      criterion: ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']
  Random Forest Regressor:
      n_estimators: [8, 16, 32, 64, 128, 256]
  XGBRegressor:
      learning_rate: [0.1, 0.01, 0.05, 0.001]
      n_estimators: [8, 16, 32, 64, 128, 256]
  CatBoosting Regressor:
      depth: [6, 8, 10]
      learning_rate: [0.01, 0.05, 0.1]
      iterations: [30, 50, 100]
  AdaBoost Regressor:
      learning_rate: [0.1, 0.01, 0.5, 0.001]
      n_estimators: [8, 16, 32, 64, 128, 256]
  Gradient Boosting:
      learning_rate: [0.1, 0.01, 0.05, 0.001]
      subsample: [0.6, 0.7, 0.75, 0.8, 0.85, 0.9]
      n_estimators: [8, 16, 32, 64, 128, 256]
  ElasticNet:
      alpha: [0.2, 0.5, 0.7, 0.9]
      l1_ratio: [0.2, 0.5, 0.7, 0.9]
